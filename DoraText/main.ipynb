{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29428f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Setting up the environment for text generation using transformers\n",
    "# This script checks the PyTorch version and GPU availability\n",
    "# and defines a function to generate text using a pre-trained model. \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0eb08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Text Generation Function\n",
    "from transformers import pipeline\n",
    "# Load the text generation pipeline\n",
    "model = pipeline(\"text-generation\")\n",
    "def generate_text(prompt,max_length=50):\n",
    "    result = model(prompt, max_length=max_length)\n",
    "    return result[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14793f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=25) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supernova is the one of the brightest galaxies in the Universe.\n",
      "\n",
      "This galaxy is the first in the constellation of galaxies known to have a unique optical composition with an extraordinarily long period of intense light.\n",
      "\n",
      "The Hubble Space Telescope has been studying the galaxy for nearly 40 years, and it has observed its light and made pictures of the galaxy.\n",
      "\n",
      "Scientists have identified the first visible light from the galaxy in the past 100 years.\n",
      "\n",
      "The Hubble telescope is a joint project of the European Space Agency and the National Science Foundation. It uses the Hubble Space Telescope to study the composition of the galaxy.\n",
      "\n",
      "Explore further: Hubble's most powerful telescope ever discovered\n"
     ]
    }
   ],
   "source": [
    "text = \"Supernova is the one of the\"\n",
    "result = generate_text(text, max_length=25)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89051b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hubble Space Telescope has been studying the galaxy for nearly 40 years. It has observed its light and made pictures of the galaxy. Supernova is the one of the brightest galaxies in the Universe.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def summarize_text(text, max_length=130, min_length=40):\n",
    "    summary = summarizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "result_summary = summarize_text(result)\n",
    "print(result_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bbd413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity_group': 'MISC', 'score': np.float32(0.38851184), 'word': 'Super', 'start': 0, 'end': 5}, {'entity_group': 'MISC', 'score': np.float32(0.8392791), 'word': 'Universe', 'start': 54, 'end': 62}, {'entity_group': 'ORG', 'score': np.float32(0.79329664), 'word': 'Hubble Space Telescope', 'start': 225, 'end': 247}, {'entity_group': 'MISC', 'score': np.float32(0.71022725), 'word': 'Hu', 'start': 457, 'end': 459}, {'entity_group': 'ORG', 'score': np.float32(0.8357145), 'word': '##bble', 'start': 459, 'end': 463}, {'entity_group': 'ORG', 'score': np.float32(0.99656385), 'word': 'European Space Agency', 'start': 500, 'end': 521}, {'entity_group': 'ORG', 'score': np.float32(0.99839514), 'word': 'National Science Foundation', 'start': 530, 'end': 557}, {'entity_group': 'MISC', 'score': np.float32(0.70515746), 'word': 'Hu', 'start': 571, 'end': 573}, {'entity_group': 'ORG', 'score': np.float32(0.5874928), 'word': '##bble Space Tel', 'start': 573, 'end': 587}, {'entity_group': 'MISC', 'score': np.float32(0.5682578), 'word': '##es', 'start': 587, 'end': 589}, {'entity_group': 'ORG', 'score': np.float32(0.8580706), 'word': 'Hubble', 'start': 652, 'end': 658}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anger/miniconda3/envs/genAI/lib/python3.10/site-packages/transformers/pipelines/token_classification.py:186: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# named entity recognition\n",
    "ner_pipeline = pipeline(\"ner\", grouped_entities=True, device=device)\n",
    "\n",
    "# grouped_entities=True to group tokens into entities if false each token is treated separately\n",
    "# Advantages of grouped_entities=True:\n",
    "# 1. Improved Readability: Grouped entities provide a clearer and more concise representation of\n",
    "#    named entities in the text, making it easier to understand the context.\n",
    "# 2. Reduced Redundancy: By grouping tokens into entities, it reduces redundancy in the output,\n",
    "#    avoiding multiple entries for the same entity.\n",
    "# 3. Better Contextual Understanding: Grouping helps in capturing the full context of an entity,\n",
    "#    which can be crucial for accurate interpretation and analysis.\n",
    "\n",
    "def recognize_entities(text):\n",
    "    entities = ner_pipeline(text)\n",
    "    return entities\n",
    "\n",
    "entities_result = recognize_entities(result)\n",
    "print(entities_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce6b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01056fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
